{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etCwX58PmT2V"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import  SubsetRandomSampler\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmLEn8PrmWKW"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "valid_size = 0.2\n",
    "num_workers= 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                         [0.5, 0.5, 0.5])\n",
    "  ])\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                           download=True, transform=transform)\n",
    "\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                           download=True, transform=transform)\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "J9vR13jnmeKr",
    "outputId": "e76efeca-53fb-4e85-8d17-8b63c9d5a2d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5463392400>"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMMAAADDCAYAAAA/f6WqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYSASGAYigWEgEhgGIuFfqsQaimwEQXsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f546367fb38>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (3,3)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvXQ_0i8mhDw"
   },
   "outputs": [],
   "source": [
    "class WhyteNetwork(nn.Module):\n",
    "  \n",
    "  def __init__(self):\n",
    "    \n",
    "    super(WhyteNetwork, self).__init__()\n",
    "    \n",
    "    hidden_1 = 258\n",
    "    hidden_2 = 128\n",
    "    hidden_3 = 64\n",
    "    \n",
    "    #The first convolution layer with 16 feature map\n",
    "    self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "    #The secound convolutional layer with 32 fature map\n",
    "    self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "    #The is maxpooling layer it help to reduce the x and y dimension of the output\n",
    "    #image size of the secound convolution layer\n",
    "    self.maxpool = nn.MaxPool2d(2,2)\n",
    "    \n",
    "    self.fc_1 = nn.Linear(32*7*7, hidden_1)\n",
    "    self.fc_2 = nn.Linear(hidden_1, hidden_2)\n",
    "    self.fc_3 = nn.Linear(hidden_2, hidden_3)\n",
    "    self.fc_4 = nn.Linear(hidden_3, 10)\n",
    "    \n",
    "    #Here implemented dropout for the fully connected layers because it help the\n",
    "    #to reduce the dead relu problems\n",
    "    self.dropout1 = nn.Dropout(p = 0.2)\n",
    "    self.dropout2 = nn.Dropout(p = 0.4)\n",
    "    self.dropout3 = nn.Dropout(p = 0.2)\n",
    "    \n",
    "      \n",
    "  def forward(self, x):\n",
    "    \n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.maxpool(F.relu(self.conv2(x)))\n",
    "    \n",
    "    x = x.view(-1, 32*7*7)\n",
    "    x = self.dropout1(F.relu(self.fc_1(x)))\n",
    "    x = self.dropout2(F.relu(self.fc_2(x)))\n",
    "    x = self.dropout3(F.relu(self.fc_3(x)))\n",
    "    x = F.log_softmax(self.fc_4(x), dim=1)\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "BV9uSm34mrAp",
    "outputId": "aabe4c91-9cdd-44dd-ccd0-a80646016702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhyteNetwork(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_1): Linear(in_features=1568, out_features=258, bias=True)\n",
      "  (fc_2): Linear(in_features=258, out_features=128, bias=True)\n",
      "  (fc_3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout1): Dropout(p=0.2)\n",
      "  (dropout2): Dropout(p=0.4)\n",
      "  (dropout3): Dropout(p=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = WhyteNetwork().cuda()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "tRrwwfDVmub6",
    "outputId": "834206aa-68db-47bb-e189-e0e357c4c8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.478653 \tValidation Loss: 0.045936\n",
      "Validation loss decreased (inf --> 0.045936).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.172541 \tValidation Loss: 0.027342\n",
      "Validation loss decreased (0.045936 --> 0.027342).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.119284 \tValidation Loss: 0.020257\n",
      "Validation loss decreased (0.027342 --> 0.020257).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.101778 \tValidation Loss: 0.018270\n",
      "Validation loss decreased (0.020257 --> 0.018270).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.088948 \tValidation Loss: 0.017604\n",
      "Validation loss decreased (0.018270 --> 0.017604).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.076659 \tValidation Loss: 0.017158\n",
      "Validation loss decreased (0.017604 --> 0.017158).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.068974 \tValidation Loss: 0.015618\n",
      "Validation loss decreased (0.017158 --> 0.015618).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.066904 \tValidation Loss: 0.014002\n",
      "Validation loss decreased (0.015618 --> 0.014002).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.061590 \tValidation Loss: 0.014891\n",
      "Epoch: 10 \tTraining Loss: 0.057025 \tValidation Loss: 0.013827\n",
      "Validation loss decreased (0.014002 --> 0.013827).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.054738 \tValidation Loss: 0.012221\n",
      "Validation loss decreased (0.013827 --> 0.012221).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.052614 \tValidation Loss: 0.012910\n",
      "Epoch: 13 \tTraining Loss: 0.047931 \tValidation Loss: 0.011962\n",
      "Validation loss decreased (0.012221 --> 0.011962).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.047900 \tValidation Loss: 0.011608\n",
      "Validation loss decreased (0.011962 --> 0.011608).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.045764 \tValidation Loss: 0.012496\n",
      "Epoch: 16 \tTraining Loss: 0.044615 \tValidation Loss: 0.011495\n",
      "Validation loss decreased (0.011608 --> 0.011495).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.042755 \tValidation Loss: 0.011972\n",
      "Epoch: 18 \tTraining Loss: 0.042370 \tValidation Loss: 0.009538\n",
      "Validation loss decreased (0.011495 --> 0.009538).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.038886 \tValidation Loss: 0.012172\n",
      "Epoch: 20 \tTraining Loss: 0.039397 \tValidation Loss: 0.013262\n",
      "Epoch: 21 \tTraining Loss: 0.039734 \tValidation Loss: 0.010718\n",
      "Epoch: 22 \tTraining Loss: 0.036541 \tValidation Loss: 0.010968\n",
      "Epoch: 23 \tTraining Loss: 0.033413 \tValidation Loss: 0.011745\n",
      "Epoch: 24 \tTraining Loss: 0.034076 \tValidation Loss: 0.010682\n",
      "Epoch: 25 \tTraining Loss: 0.032504 \tValidation Loss: 0.011698\n",
      "Epoch: 26 \tTraining Loss: 0.033742 \tValidation Loss: 0.011652\n",
      "Epoch: 27 \tTraining Loss: 0.031928 \tValidation Loss: 0.010835\n",
      "Epoch: 28 \tTraining Loss: 0.029792 \tValidation Loss: 0.011747\n",
      "Epoch: 29 \tTraining Loss: 0.031865 \tValidation Loss: 0.010968\n",
      "Epoch: 30 \tTraining Loss: 0.032170 \tValidation Loss: 0.010753\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'childrenModel.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1e2HFw__m5g2"
   },
   "outputs": [],
   "source": [
    "#Here is a function that helps to load in our checkpoint\n",
    "def load_checkpoint(filepath):\n",
    "  \n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vr4j-BC_Dw0c"
   },
   "outputs": [],
   "source": [
    "#Here i will load in my checkpoint the code here\n",
    "model = load_checkpoint('childrenModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "fh7l3SjIm_ol",
    "outputId": "9ee7c77e-b4c1-4881-cc84-512b8c8a5dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.045507\n",
      "\n",
      "Test Accuracy of     0: 100% (241/241)\n",
      "Test Accuracy of     1: 98% (285/288)\n",
      "Test Accuracy of     2: 97% (256/263)\n",
      "Test Accuracy of     3: 98% (257/261)\n",
      "Test Accuracy of     4: 98% (267/270)\n",
      "Test Accuracy of     5: 96% (209/216)\n",
      "Test Accuracy of     6: 97% (218/223)\n",
      "Test Accuracy of     7: 99% (261/263)\n",
      "Test Accuracy of     8: 99% (222/223)\n",
      "Test Accuracy of     9: 97% (258/264)\n",
      "\n",
      "Test Accuracy (Overall): 98% (2474/2512)\n"
     ]
    }
   ],
   "source": [
    "#The code below is used to test the performance of the model on the test data\n",
    "\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(16):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7ocmbFqoTC2"
   },
   "outputs": [],
   "source": [
    "#The function below helps to process the image so it can be passed to the model\n",
    "def process_image(img_path, max_size=28, shape=None):\n",
    "    ''' Load in and transform an image, making sure the image\n",
    "       is = 28 pixels in the x-y dims.'''\n",
    "    \n",
    "    image = Image.open(img_path).convert('LA')\n",
    "    \n",
    "    # large images will slow down processing\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5, 0.5, 0.5), \n",
    "                                             (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :1) and add the batch dimension\n",
    "    image = in_transform(image)[:1,:,:].unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVz264YeGvwK"
   },
   "outputs": [],
   "source": [
    "#The function here helps to get prediction from the model\n",
    "def predict(image_path, model, topk_value):\n",
    "  \n",
    "    image = process_image(image_path, shape=(28, 28))\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    model = model.type(torch.FloatTensor)\n",
    "    model.eval()\n",
    "    logps = model(image).to(device)\n",
    "    ps = torch.exp(logps)\n",
    "    probs, classes = ps.topk(topk_value, dim=1)\n",
    "    probs = Variable(probs, requires_grad= False)\n",
    "    probs = probs.type(torch.FloatTensor).numpy().squeeze()\n",
    "    classes = Variable(classes, requires_grad = False)\n",
    "    classes = classes.type(torch.FloatTensor).numpy().squeeze()\n",
    "\n",
    "    return probs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHOY-WKTIJM_"
   },
   "outputs": [],
   "source": [
    "#The function here helps to plot the predicted results for visulization purposes\n",
    "def display_images(image_path, model, topk_value):\n",
    "    probs, classes = predict(image_path, model, topk_value)\n",
    "    disp_image = Image.open(image_path)\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 10))\n",
    "\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax1.imshow(disp_image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(image_path)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "\n",
    "    ax2_label = []\n",
    "    for name in classes:\n",
    "        ax2_label.append(str(int(name)))\n",
    "    ax2.barh(ax2_label, probs)\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "colab_type": "code",
    "id": "IheuC4VbIV4U",
    "outputId": "71142226-4aeb-4e5b-a1c4-6f31c829885a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAJNCAYAAACyWvlMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3WlclPX+//H3AFKhaIKg9OiYaVmp\n2eaSSpILAS6dbNNy6Vinc6wsbTMzU9QWs9TSY4tlu8c2OSfLfSn1uJBbmVqpdSxcUlRCFDsIzP+G\nP+cvpsyAM3PNB17PW+HMdc1nrvDl9b0YL11ut9stADAszOkBAOB0ETIA5hEyAOYRMgDmETIA5hEy\nAOYRsjJavXq12rdvf9LH3n//fb344ountf8777xTGzdu1ODBg9WkSRP99ttvf3j9iy66SBkZGT7v\nMzMzU02aNFFqaqquu+46tWvXTkOGDNHu3btPuc2gQYO0aNGicr+PgoICJSUlqbi4uNz7CAXt27fX\n6tWrT3s/+/btU9++fZWcnOyHqXCiCKcHqEh69ep1WtsXFBToxx9/VKNGjSRJtWrV0ty5c9W9e3fP\nc2bOnKmEhIQy7zshIUFz5syRJP3vf//T66+/ru7duysjI0MxMTF/eP6YMWPK+S6O+vrrr3XppZcq\nLIw/K3/77Tf16tVLbdu21fbt250ep0Liu8wHL7/8spKSknTDDTdo+fLlnl+fOHGihg4dqptvvllv\nv/22Jk6cqCeeeEJTp05Vv379PM8rKipSy5Yt9eOPP+rXX39Vv379lJKSopSUFC1evNjzvDVr1ujy\nyy+Xy+WSJLVt21aff/55if0sXbpUV155pSRp6tSp+vvf/+55vLi4WK1bt9Z3331X6vs544wz1L9/\nfzVr1kxvv/22JKl3794aP3680tLStHbtWvXu3VuffvqpBgwYoDfffNOz7XfffafExEQVFxdrzZo1\nuummm5ScnKxbb71VWVlZnuctX75crVq1knT0jLBbt25KTU3VLbfcom+//VaSlJGRoQceeEBDhgxR\nSkqKOnXqpC1btkiSDhw4oEcffVQpKSnq0KGDpk+fftL3sn37diUmJurdd99V165ddc0112jWrFme\n/z9PPPFEif9fx77u3bu3Jk+erO7du+vqq6/W1KlT9fLLLys1NVWdOnUq8V5WrlypG264QUlJSRo/\nfrzn1xcsWKCuXbuqQ4cOuvPOO7V//37P6xz/feFyuTRp0qRTnskfk5GRobvvvluPPvqoOnbsqC5d\numjbtm2SpMGDB2vChAnq27ev2rVrp759++rw4cOSpKVLlyopKUlpaWn68MMPdeWVV1a6YBIyL7Zu\n3aq3335b06dP1/Tp0/XDDz+UeHzx4sWaPHmy/vKXv3h+7brrrlNmZqbnG23VqlWKj49XgwYN9Nhj\nj+niiy/W3LlzNXnyZA0aNEg5OTmSpGXLlnl+80vSZZddph07dniWgCtWrFDTpk0VGRkpSUpNTdXK\nlSs9269du1bVq1fXJZdc4tN7a9++vTIzMz1fb9iwQTNnzvSEUpJSUlJKLDHnz5+v1NRU5efn6557\n7tFDDz2k+fPnq0+fPhowYIDnecdCdujQIQ0YMEBDhw7VnDlz9Ne//lWPPPKIZ8m5ZMkS3X777Zo7\nd65atmypd955R5I0evRohYWFafbs2fr44481ceJEbd68+aTvIycnR2FhYfrss880ZMgQn5f3q1at\n0tSpU/Xss8/q+eefV506dTRnzhxdcMEFJcK5ceNGTZ8+XRkZGZo2bZq+//57ZWVladCgQRo7dqwW\nLlyoli1bKj093bPN8d8XNWrUUP369X2aafny5erZs6cWLFigDh066Pnnn/c8NmfOHI0fP17z58/X\n/v37NX/+fBUVFWnw4MEaOXKkZs+erW3btnm+7yoTQubFqlWr1Lx5c9WqVUvh4eG6/vrrSzx+2WWX\n/WFpFhcXp0aNGmnZsmWSjv7JnZaWpvz8fGVmZnqid9555+mqq67ynJUtX75crVu39uzH5XIpJSVF\nM2fOlHR0WdmpUyfP47GxsWrWrJnmzp0r6Whkjn/cm2rVqikvL8/zdVJS0h+Wgtdee602bdrkuVZ3\nLGRr1qxR7dq11aZNG0lSly5d9Msvv2jnzp3Ky8vT3r17Vb9+fa1fv1516tTRVVddJeloGHNycrRj\nxw5JUoMGDdSkSRNJUqNGjbRr1y5J0hdffKE+ffooLCxMMTExSk5O1rx58076PgoLC3XjjTdKkho3\nbqydO3f69P7btWuniIgINWzYUIcPH1ZKSookqWHDhtqzZ4/neV27dlV4eLhiY2PVvHlzrVu3TkuW\nLFGLFi3UsGFDSVKPHj20aNEiFRUVSTr594UvGjRooMsvv1zS0WO1bt06z2NJSUk6++yzPTPv2rVL\n27Zt81yPlI6eaVq/LlkeXCPzIjc3V9HR0Z6vq1evXuLxGjVqnHS7Y2cyHTt21MKFC/XWW28pLy9P\nbrdbPXr08DwvPz9fV199tXJycnTgwAHVrVu3xH66dOmiJ598Ur169VJmZqZGjBihBQsWeB7v3Lmz\nMjIy1KNHDy1cuFCvvvqqz+9tx44dio2NLfW9REVFqXXr1vryyy911VVX6cCBA7rqqqv0+eefKysr\nS6mpqZ7nRkZGav/+/dq4caNatGghSdq/f/8fjll0dLT27dvn+e9jwsPDPSHIy8vTwIEDFR4eLuno\ndb3U1FTNnz9fY8eOlXT0muS1116r8PBwRUVFSZLCwsJ8/o1ctWpVz+se//WJ+zg+SNHR0Tpw4IDc\nbrdWr15d4v1Xq1bNE/xTfV94c/x21atX14EDB0q89jHHjlVubm6J4xsfH1+u17WOkHlRvXr1Emct\nx5Zx3qSkpOi1117Tt99+qxo1aqhevXoqLCxUeHi4pk+f7vlNc8zs2bPVsmXLP+yncePGOnTokD76\n6CM1b97cs6w8Jjk5WSNHjtTixYt11lln6YILLvD5vc2dO9dzRuXtvcyfP185OTlKSUmRy+VSfHy8\n6tevf9Kfnk6fPt2zRI6NjS3xk1e3263c3FzFxsbqp59+OuVrxsfHa9KkSZ4znuMd/5O/0q4FnRik\n3Nzc0t/oKRy/XW5urmrUqKHIyEi1bt1aEyZMKNc+T+X4Y3XstUpTrVo15efne77eu3evX+exgqWl\nF1dccYXWrFmj/fv3q6ioSDNmzPBpu9q1a+tPf/qTXn31VaWlpUmSIiIilJSUpA8++ECSdPjwYT3+\n+OPatWtXiYvjJ+rcubNeeeWVky4bo6Ojdc0112jEiBGe1/GmoKBAL774orZv366ePXt6fX67du20\nbt06zxJZOrp0ys7O1jfffCNJysrK0qOPPiq3213ivTRt2lR79+71LJFmzpypOnXq6Nxzzy31Ndu3\nb+85ToWFhXrmmWe0ceNGn97fMfHx8dq8ebOKi4u1f/9+LVmypEzbHzNz5kwVFxdr3759WrNmjZo1\na6bExEStXr3a80OB9evX66mnnirX/o/33//+V5s2bZJ09A+aY0vyUzn2B+Sxa53Tpk3z/LCoMuGM\nzItLLrlEPXr0ULdu3XT22Werc+fOp7zofKKUlBSNHj1ajz32mOfX0tPTNXz4cH388ceSpOuvv14J\nCQlauXKlBg4ceNL9dO7cWVOnTi1x/ezEx+fNm1fq9bFdu3YpNTVVbrdbhw4dUqtWrTR16tQSy5VT\nqVatmho3bqwffvjBc/3mzDPP1IQJEzRq1CgdOnRIVapU0YABA7Rr1y5FRER4ljhRUVF68cUXNWrU\nKOXn5ysmJkbjxo3z+ptt4MCBGjFihOe61TXXXKOLLrrI66zHS01N1YwZM9SxY0fVr19fqampniVt\nWVx66aW6+eabtX//ft1xxx2es95Ro0bpvvvu05EjR1S1alUNGTLkpNsvWrRIY8aM0e+//669e/cq\nNTVVtWvX1jvvvKP58+dr0aJFevbZZyUd/YPz7bff1urVqxUVFaVXXnml1NkiIyOVnp6uxx9/XNHR\n0erbt6/CwsIqXcxc3I/MvvXr12vkyJH65JNPnB4FpyEjI0MzZszwfCSmPPLz83XFFVdo9erVPv0h\nVVGwtDSusLBQkyZNUu/evZ0eBQ656aabPJ+dmzVrlho0aFCpIiYRMtM2bdqk5ORkxcfH/+FjIag8\nHn/8cb366qtKSUnRP//5T40ePdrpkYKOpSUA8zgjA2AeIQNgHiEDYB4hA2AeIQNgHiEDYB4hA2Ae\nf9cSFU5BQYHuueceud1uzy16pKO3BmrRooUeeughB6dDIPCBWFQov//+u1JSUk55p4tWrVqVuF05\nKgaWlqgQ7r33XrlcLp111lml3q7n2A0YUbEQMpg1YcIEuVwuuVwur7e7QcXGNTKY8+233+qVV14h\nXvAgZDAhLy9PU6ZM0YMPPuj0KAhBhAwhbcGCBercubMKCgqcHgUhjJAhJP3222/q0KGD1q5d6/Qo\nMICL/QgZbrdbt912m1wul2rWrEnE4DNCBsc99dRTcrlcCgsL8/zLSUBZsLSEY7766itNmTJFkydP\ndnoUGEfIEFR79+7Vu+++q4cfftjpUVCBEDIEnNvt1uzZs9W5c2enR0EFRcgQUDk5OWrTpo2+++47\np0dBBcbFfgREly5d5HK5FBMTQ8QQcIQMfjNo0CDP332cOXOm0+OgEmFpidP2n//8Ry+//LKmTZvm\n9CiopAgZyuWXX37RW2+9pfT0dKdHAQgZym758uVKTk5Wfn6+06MAkggZfLR7924lJiZq69atTo8C\n/AEX+3FKxcXFSktLk8vlUp06dYgYQhYhw0mNHz9ejRo10pw5c5weBfCKpSU8Zs+erX/84x+aNWuW\n06MAZULIKrnNmzfrtdde07hx45weBSg3QlaJLV68WO3bt1dxcbHTowCnhZBVMr/88ovatGmj7du3\nOz0K4Ddc7K8E8vPz1bZtW7lcLp133nlEDBUOIavgRo4cqaZNm2rp0qVOjwIEDEvLCigjI0OTJk3S\nokWLnB4FCApCVkFs2LBBb7zxhl566SWnRwGCjpBVADNnztT111/PTx9RaREyo3744QclJiZq7969\nTo8COI6L/QYtWrRITZo0IWLA/yFkRgwbNkznnHOOXC6XOnTooMLCQqdHAkIGS8sQ9tFHH+mll17S\n8uXLnR4FCGmELESlp6drxIgRTo8BmEDIQsi0adN0++23Oz0GYA4hCwGRkZE6cuRI0F4vKipKxcXF\n+v3334P2mkAgEbIQEMiIRUdHq27dukpNTVX9+vXVunVrxcbGyu12a9euXZo6daqWLl2qr7/+OmAz\nAIFGyCqY5ORkDR06VI0bN1ZsbGypz61bt65atmzp+TopKUlLliwJ9IiA3xEyw84880w1b95cEydO\n1GWXXXba+1u8eLEkyeVynfa+gGDic2RGdenSRRs2bNCSJUv8ErHj8dNSWEPIjOjQoYNefPFFud1u\nud1uffbZZ2rQoEFAXmvYsGE699xzA7JvIBBYWoawu+66S3fffXeJ61jBkpWVpWbNmmnNmjVBf22g\nrDgjC1H33nuv3njjDUcidsyDDz7o2GsDZUHIQsjQoUM9S8dJkyY5PY569uzp9AiAT1hahgC32+30\nCKfUpk0bLVu2zOkxgFJxRoZStW7d2ukRAK8IGUo1ePBgxcfHOz0GUCpChlLFxMSoWbNmTo8BlIqQ\nwSsu+iPUETJ4lZiY6PQIQKkIGbxKSEhwegSgVIQMXlWpUsXpEYBSETIA5hEyAOYRMgDmETL4JDIy\n0ukRgFMiZPBJeHi40yMAp0TI4JOwML5VELr47oRPOCNDKCNkAMwjZADMI2QAzCNkAMwjZADMI2QA\nzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADM\nI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwj\nZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNk\nAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QA\nzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADM\nI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwj\nZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNk\nAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QA\nzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZPBJ\ncXGx0yP4hcvlcnoEBAAhg08KCwudHsEvDh065PQICABCBsA8QgafFBUVOT2CXxw+fNjpERAAhAw+\nOXLkiNMj+AUhq5gIGSqVinJmiZIIGSqVvLw8p0dAABAyVCoHDx50egQEACFDpVJQUOD0CAgAQgav\nKsqHYaWK80MLlETI4FV2drbTIwClImTw6quvvnJ6BKBUhAxeLViwwOkR/IrPklU8hAylKi4u1vr1\n650ew6/27Nnj9AjwM0KGUi1btkxffvml02P41fLly50eAX5GyFCqMWPGOD2C37388stOjwA/I2Qo\nVUVbVkrSjz/+6PQI8DOX2+12Oz0EQldFvREh3/YVC2dkOKX33nvP6RECZtWqVU6PAD8iZDilyZMn\nOz1CwMydO9fpEeBHLC1xUt26ddO///1vp8cIqDfffFN9+/Z1egz4ASHDHxw5ckSRkZFOjxEUfPtX\nDCwt8QejRo1yeoSg4VP+FQNnZCjh559/Vr169ZweI6j4LWAfZ2TwKCgoUI8ePZweI+i2b9/u9Ag4\nTZyRQdLRJVZUVJTTYzgmLS1Ns2bNcnoMlBNnZNB7771XqSMmSbNnz9b555/v9BgoJ0JWyaWkpKhP\nnz5OjxEStm3bpvT0dK6ZGUTIKqGdO3fqsccek8vl0rx585weJ6SMGDFCYWFh6tixoxYuXOj0OPAR\n18gqiXnz5mnevHkaO3as06OY07dvX11//fW64YYbnB4Fp0DIKrC8vDx9+OGHWrRokaZNm+b0OObd\neOONGjZsmOrVq6caNWo4PQ6OQ8iMKioq0q+//qpDhw5p8+bNWrp0qbZt26YVK1YoKyvL6fEqhbPO\nOktxcXFq3bq1kpKSlJCQoMaNG6tq1apKSEhwerxKhZCFgGeffVajRo1SXFycCgoKVFxcrIKCAhUU\nFOj333+vUP8cW2VXpUoVRUVFKSIiQmeeeaaioqLUrVs3Pffcc06PZlqE0wNAysrK0uHDh/XLL784\nPQoC7MiRI8rNzS3xaz/99JND01Qc/NQyBJx11llOjwCHuFyuSv8ZPn8gZADMI2QAzCNkAMwjZADM\nI2QhgJv7VV5ut1s5OTlOj2EenyMDYB5nZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNk\nAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QA\nzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADM\nI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwj\nZADMI2QAzCNkAMwjZADMI2QAzIvw9w4LC4uUk5Pv790GRc2aUczuEMvzM3twxMVFn/Ixv5+RRUSE\n+3uXQcPszrE8P7M7j6UlAPMIGQDzCBkA8wgZAPMIGQDzCBkA8wgZAPMIGQDzCBkA8/z+V5S6Pvyp\nv3eJMnhzcHunRwCCjjMyAOYRMgDmETIA5hEyAOYRMgDmETIA5hEyAOYRMgDmETIA5hEyAOYRMgDm\nETIA5hEyAOYRMgDmETIA5nm9H1lxcbGGDx+uLVu2qEqVKkpPT1eDBg2CMRsA+MTrGdnChQuVl5en\nDz74QE8//bTGjBkTjLkAwGdeQ7Zt2zY1bdpUklS3bl3t3LlTRUVFAR8MAHzldWnZsGFDvfPOO7rj\njjv0888/KysrSzk5OapVq1Yw5kMZxcVFOz1CuTG7MyzPfozXkCUlJWnt2rXq2bOnLrroItWvX19u\ntzsYs6EcsrPznB6hXOLiopndAZZmLy24Pv3jIw8++KDnvzt27KjY2NjTnwoA/MTrNbLvv/9ejz/+\nuCRpyZIlatSokcLC+NQGgNDh0zUyt9utm2++WWeccYZeeOGFYMwFAD7zGrKwsDCNHj06GLMAQLmw\nRgRgHiEDYB4hA2AeIQNgHiEDYB4hA2AeIQNgHiEDYB4hA2AeIQNgHiEDYB4hA2CeT/cjK4vPxv7Z\nzI3aTmTpJnMnsjw7cLo4IwNgHiEDYB4hA2AeIQNgHiEDYB4hA2AeIQNgHiEDYB4hA2Ce3z/Z3/Xh\nT/29S8Cv3hzc3ukR4GeckQEwj5ABMI+QATCPkAEwj5ABMI+QATCPkAEwj5ABMI+QATCPkAEwj5AB\nMI+QATCPkAEwj5ABMM/rbXwOHTqkxx57TLm5uTpy5Ijuu+8+XXPNNcGYDQB84jVk//rXv3T++efr\n4Ycf1u7du3XHHXdozpw5wZgNAHzidWlZs2ZN/fbbb5KkAwcOqGbNmgEfCgDKwusZWefOnZWRkaHk\n5GQdOHBAr732WjDmAgImLi7axD6DxfLsx3gN2aeffqpzzjlHU6ZM0ffff68hQ4YoIyMjGLMBAZGd\nnefX/cXFRft9n8FiafbSgut1abl27VolJiZKki6++GLt2bNHRUVF/psOAE6T15Cdd955+uabbyRJ\nO3bsUNWqVRUeHh7wwQDAV16Xlt27d9eQIUPUq1cvFRYWKj09PQhjAYDvvIasatWqeumll4IxCwCU\nC5/sB2AeIQNgHiEDYB4hA2AeIQNgHiEDYB4hA2AeIQNgHiEDYB4hA2AeIQNgHiEDYB4hA2Ce17tf\nlNVnY/9s5o6TJ7J0t8wTWZ5dsj8/nMUZGQDzCBkA8wgZAPMIGQDzCBkA8wgZAPMIGQDzCBkA8wgZ\nAPP8/sn+rg9/6u9dAif15uD2To+AEMEZGQDzCBkA8wgZAPMIGQDzCBkA8wgZAPMIGQDzCBkA8wgZ\nAPMIGQDzCBkA8wgZAPMIGQDzCBkA87zexufjjz/WjBkzPF9v2LBB69atC+hQAFAWXkN2yy236JZb\nbpEkffXVV5o9e3bAhwKAsijT0nLSpEm69957AzULAJSLz3eIXb9+vRISEhQXFxfIeQCfxcVFOz2C\nRyjNUlaWZz/G55B98skn6tatWyBnAcokOzvP6REkHQ1BqMxSVpZmLy24Pi8tMzMzdcUVV/hlIADw\nJ59Ctnv3blWtWlWRkZGBngcAysynkGVnZysmJibQswBAufgUsiZNmuiNN94I9CwAUC58sh+AeYQM\ngHmEDIB5hAyAeYQMgHmEDIB5hAyAeYQMgHmEDIB5hAyAeYQMgHmEDIB5hAyAeT7fIdZXn439s5k7\nTp7I0t0yT2R5dsn+/HAWZ2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMzz+wdiuz78qb93\nWaG8Obi90yMAFQ5nZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNk\nAMwjZADMI2QAzCNkAMzzej+yzMxMDRgwQBdeeKEkqWHDhnryyScDPhgA+MqnGyu2aNFCEyZMCPQs\nAFAuLC0BmOfTGdnWrVvVr18/5ebmqn///mrTpk2g56qw4uKiTe47GCzPz+zO8hqyevXqqX///kpL\nS1NWVpb69OmjefPmKTIyMhjzVTjZ2XkB2W9cXHTA9h0Mludn9uAoLbhel5a1a9dWp06d5HK5VLdu\nXdWqVUu7d+/264AAcDq8hmzGjBmaMmWKJCk7O1v79u1T7dq1Az4YAPjK69Kyffv2euSRR7Rw4UId\nOXJE6enpLCsBhBSvIatWrZpeffXVYMwCAOXCxy8AmEfIAJhHyACYR8gAmEfIAJhHyACYR8gAmEfI\nAJhHyACYR8gAmEfIAJhHyACYR8gAmOfTra7L4rOxfzZzx8kTWbpbJoD/jzMyAOYRMgDmETIA5hEy\nAOYRMgDmETIA5hEyAOYRMgDm+f0DsV0f/tTfuwRQwbw5uL1f98cZGQDzCBkA8wgZAPMIGQDzCBkA\n8wgZAPMIGQDzCBkA8wgZAPMIGQDzCBkA8wgZAPMIGQDzCBkA8wgZAPN8CtnmzZvVsWNHvf/++4Ge\nBwDKzGvI8vPzNWrUKLVq1SoY8wBAmXkNWWRkpF5//XXFx8cHYx4AKDOvt7qOiIhQRITf74gNoBKL\ni4v26/4oFICgy87OK/M2pcWPn1oCMI+QATDP69Jyw4YNeu6557Rjxw5FRERo7ty5mjhxos4+++xg\nzAcAXnkNWZMmTfTee+8FYxYAKBeWlgDMI2QAzCNkAMwjZADMI2QAzCNkAMwjZADMI2QAzCNkAMwj\nZADMI2QAzCNkAMwjZADMc7ndbre/d1qeuz+Ggri4aGZ3iOX5mT04uEMsgAqNkAEwj5ABMI+QATCP\nkAEwj5ABMI+QATCPkAEwj5ABMI+QATAvIH9FCQCCiTMyAOYRMgDmETIA5hEyAOYRMgDmETIA5hEy\nAOZFlOXJzzzzjL755hu5XC4NGTJETZs29Ty2fPlyjRs3TuHh4Wrbtq3uu+8+r9sEW1nnz8zM1IAB\nA3ThhRdKkho2bKgnn3wy5Gb/3//+p2HDhmnLli3KyMjwaZtgKuvsVo77ypUrNW7cOIWFhen888/X\n008/rbCwsJA57uWZf9WqVSFz7MvE7aPMzEz33/72N7fb7XZv3brVfeutt5Z4PC0tzb1z5053UVGR\n+7bbbnNv2bLF6zbBVJ75V65Uar1FAAAC0klEQVRc6b7//vudGLcEb7OPHDnS/dZbb7m7devm8zbB\nUp7ZrRz35ORk965du9xut9t9//33u7/88suQOe5ud/nmD5VjX1Y+Ly1XrFihjh07SpIaNGig3Nxc\nHTx4UJKUlZWlGjVqKCEhQWFhYUpKStKKFStK3SbYyjN/qPB2HB988EHP475uEyzlmT1UeJs9IyND\nderUkSTFxMQoJycnZI67VL75rfI5ZHv37lXNmjU9X8fExCg7O1uSlJ2drZiYmD88Vto2wVae+SVp\n69at6tevn2677TYtW7YsuEP/H2/HsVq1amXeJljKM7tk67jv2bNHy5YtU1JSUsgcd6l880uhcezL\nqkzXyI7nLsdf0SzPNoHiyyz16tVT//79lZaWpqysLPXp00fz5s1TZGRkECY8NcvHvqId93379qlf\nv34aPnx4iWiUto1TfJk/VI+9Nz6fkcXHx2vv3r2er/fs2aO4uLiTPrZ7927Fx8eXuk2wlWf+2rVr\nq1OnTnK5XKpbt65q1aql3bt3h9Ts/twmEMozh5XjfvDgQd19990aOHCgEhMTfdommMozf6gc+7Ly\nOWRt2rTR3LlzJUkbN25UfHy859T03HPP1cGDB7V9+3YVFhbqiy++UJs2bUrdJtjKM/+MGTM0ZcoU\nSUeXn/v27VPt2rVDanZ/bhMI5ZnDynEfPXq07rjjDrVt29bnbYKpPPOHyrEvqzLdxueFF17Q6tWr\n5XK5NHz4cG3atEnR0dFKTk7WqlWr9MILL0iSrrvuOt11110n3ebiiy8OzDsJwPwHDx7UI488ogMH\nDujIkSPq37+/5zpCKM3+wAMP6Ndff9WWLVvUpEkT3XrrreratWvIHPuyzt6uXbuQP+6JiYlq3ry5\nrrjiCs9zu3Tpou7du4fMcS/P/J07dw6ZY18W3I8MgHl8sh+AeYQMgHmEDIB5hAyAeYQMgHmEDIB5\nhAyAef8P35aLjXdF/RIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f546351e1d0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_images('drive/My Drive/one-number1.png', model, 5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Abimbola's little side project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
